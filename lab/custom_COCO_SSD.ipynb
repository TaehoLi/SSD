{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"utils\")\n",
    "from make_txt import make_txt\n",
    "os.chdir(\"..\")\n",
    "\n",
    "folder = \"/home/user/ssd1/taeholee/SSD/coco-ssd/data/\"\n",
    "annotations = \"/home/user/hdd/coco/annotations/\"\n",
    "\n",
    "make_txt(100, txt_folder=folder, annotations=annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "\n",
    "os.chdir(\"utils\")\n",
    "from misc import store_labels\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from ssd.ssd import MatchPrior\n",
    "from ssd.vgg_ssd import create_vgg_ssd\n",
    "from ssd.config import vgg_ssd_config\n",
    "from ssd.data_preprocessing import TrainAugmentation, TestTransform\n",
    "\n",
    "from datasets.COCO_dataset import COCODataset\n",
    "\n",
    "from nn.multibox_loss import MultiboxLoss\n",
    "\n",
    "from train_and_test import train, test\n",
    "\n",
    "print('Single Shot MultiBox Detector Training With Pytorch')\n",
    "\n",
    "dataset_type = \"COCO\"\n",
    "datasets = \"/home/user/hdd/coco/\"\n",
    "validation_dataset = \"/home/user/hdd/coco/\"\n",
    "txt_folder = \"/home/user/ssd1/taeholee/SSD/coco-ssd/data/\"\n",
    "net = \"vgg16-ssd\"\n",
    "\n",
    "# Params for SGD\n",
    "lr = 1e-3 #1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "base_net_lr = None\n",
    "extra_layers_lr = None\n",
    "\n",
    "# Params for loading pretrained basenet or checkpoints.\n",
    "folder = \"/home/user/ssd1/taeholee/SSD/coco-ssd/models/\"\n",
    "base_net = folder + \"vgg16_reducedfc.pth\"\n",
    "pretrained_ssd = None\n",
    "resume = folder + \"vgg16-ssd-Epoch-49-Loss-4.91829.pth\"\n",
    "\n",
    "# Scheduler\n",
    "scheduler = \"multi-step\"\n",
    "\n",
    "# Params for Multi-step Scheduler\n",
    "milestones = \"120,160\"\n",
    "\n",
    "# Params for Cosine Annealing\n",
    "t_max = 120.0\n",
    "\n",
    "# Train params\n",
    "batch_size = 8 #24\n",
    "num_epochs = 2 #100\n",
    "num_workers = 8\n",
    "validation_epochs = 1\n",
    "debug_steps = 100\n",
    "use_cuda = True\n",
    "use_multi_gpu = True\n",
    "checkpoint_folder = folder\n",
    "#num_training_loss = 0 #in train_and_test.py\n",
    "\n",
    "\n",
    "\n",
    "###main\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.benchmark = True\n",
    "    print(\"Use Cuda.\")\n",
    "    DEVICE_IDS = [3,1] #GPU numbers\n",
    "    DEVICE = torch.device(\"cuda:3\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "if net == 'vgg16-ssd':\n",
    "    create_net = create_vgg_ssd\n",
    "    config = vgg_ssd_config\n",
    "else:\n",
    "    print(\"No network\")\n",
    "    \n",
    "train_transform = TrainAugmentation(config.image_size,\n",
    "                                    config.image_mean,\n",
    "                                    config.image_std)\n",
    "target_transform = MatchPrior(config.priors,\n",
    "                              config.center_variance,\n",
    "                              config.size_variance, 0.5)\n",
    "test_transform = TestTransform(config.image_size,\n",
    "                               config.image_mean,\n",
    "                               config.image_std)\n",
    "\n",
    "\n",
    "print(\"Prepare training and validation datasets.\")\n",
    "datalist = []\n",
    "if dataset_type == 'COCO':\n",
    "    dataset     = COCODataset(root=datasets, txt=txt_folder,\n",
    "                              transform=train_transform,\n",
    "                              target_transform=target_transform,\n",
    "                              is_test=False, is_validate=False)\n",
    "    val_dataset = COCODataset(root=validation_dataset, txt=txt_folder,\n",
    "                              transform=test_transform,\n",
    "                              target_transform=target_transform,\n",
    "                              is_test=False, is_validate=True)\n",
    "    \n",
    "    label_file = os.path.join(checkpoint_folder, \"coco-model-labels.txt\")\n",
    "    store_labels(label_file, dataset.class_names)\n",
    "    num_classes = len(dataset.class_names)\n",
    "    print(\"Stored labels into file coco-model-labels.txt.\")\n",
    "else:\n",
    "    print(\"No dataset_type\")\n",
    "\n",
    "\n",
    "datalist.append(dataset)\n",
    "train_dataset = ConcatDataset(datalist)\n",
    "\n",
    "print(\"Train dataset size: {}\". format(len(train_dataset)))\n",
    "print(\"validation dataset size: {}\".format(len(val_dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "print(\"Build network.\")\n",
    "net = create_net(num_classes, device=DEVICE, is_test=False)\n",
    "min_loss = -10000.0\n",
    "last_epoch = -1\n",
    "\n",
    "base_net_lr = base_net_lr if base_net_lr is not None else lr\n",
    "extra_layers_lr = extra_layers_lr if extra_layers_lr is not None else lr\n",
    "    \n",
    "params = [\n",
    "            {'params': net.base_net.parameters(), 'lr': base_net_lr},\n",
    "            {'params': itertools.chain(net.source_layer_add_ons.parameters(),\n",
    "                                       net.extras.parameters()),\n",
    "             'lr': extra_layers_lr},\n",
    "            {'params': itertools.chain(net.regression_headers.parameters(),\n",
    "                                       net.classification_headers.parameters())}\n",
    "         ]\n",
    "\n",
    "\n",
    "if resume:\n",
    "    print(\"Resume from the model:\", resume, \"\\n\")\n",
    "    torch.load(resume)\n",
    "    #net.load_state_dict(resume)\n",
    "    #net.load_state_dict(torch.load(resume))\n",
    "elif pretrained_ssd:\n",
    "    print(\"Init from pretrained ssd\\n\")\n",
    "    net.init_from_pretrained_ssd(pretrained_ssd)\n",
    "elif base_net:\n",
    "    print(\"Init from base net\\n\")\n",
    "    net.init_from_base_net(base_net)\n",
    "    \n",
    "\n",
    "if use_cuda and use_multi_gpu and torch.cuda.device_count() > 1: # using multi-GPU\n",
    "    print(\"Let's use\", tuple(DEVICE_IDS), \"multi-GPU number.\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    net = nn.DataParallel(net, device_ids=DEVICE_IDS, output_device=DEVICE_IDS[0])\n",
    "\n",
    "net.to(DEVICE)\n",
    "criterion = MultiboxLoss(config.priors, iou_threshold=0.5, neg_pos_ratio=3,\n",
    "                         center_variance=0.1, size_variance=0.2, device=DEVICE)\n",
    "optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "print(\"Learning rate:\", lr, \"Base net learning rate:\", base_net_lr,\n",
    "      \"Extra Layers learning rate:\", extra_layers_lr)\n",
    "\n",
    "if scheduler == 'multi-step':\n",
    "    print(\"Uses MultiStepLR scheduler.\")\n",
    "    milestones = [int(v.strip()) for v in milestones.split(\",\")]\n",
    "    scheduler = MultiStepLR(optimizer, milestones=milestones,\n",
    "                            gamma=0.1, last_epoch=last_epoch)\n",
    "elif scheduler == 'cosine':\n",
    "    print(\"Uses CosineAnnealingLR scheduler.\")\n",
    "    scheduler = CosineAnnealingLR(optimizer, t_max, last_epoch=last_epoch)\n",
    "else:\n",
    "    print(\"Unsupported Scheduler\")\n",
    "\n",
    "\n",
    "print(\"Start training from epoch 0.\\n\")\n",
    "for epoch in range(last_epoch + 1, num_epochs):\n",
    "    scheduler.step()\n",
    "    train(train_loader, net, criterion, optimizer,\n",
    "          device=DEVICE, checkpoint_folder=checkpoint_folder,\n",
    "          debug_steps=debug_steps, epoch=epoch)\n",
    "        \n",
    "    if epoch % validation_epochs == 0 or epoch == num_epochs - 1:\n",
    "        val_loss, val_regression_loss, val_classification_loss = test(val_loader,\n",
    "                                                                      net,\n",
    "                                                                      criterion,\n",
    "                                                                      DEVICE)\n",
    "        print(\"Epoch:\", epoch, \"\\n\", \n",
    "              \"Validation Loss:\", val_loss, \"\\n\",\n",
    "              \"Validation Regression Loss:\", val_regression_loss, \"\\n\",\n",
    "              \"Validation Classification Loss:\", val_classification_loss\n",
    "              )\n",
    "        \n",
    "        name = \"vgg16-ssd\"+\"-Epoch-\"+\"%s\"%epoch+\"-Loss-\"+\"%s\"%str(val_loss)[:7]+\".pth\"\n",
    "        model_path = os.path.join(checkpoint_folder, name)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        print(\"Saved model:\", model_path, \"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eval\n",
    "__'difficult':__\n",
    "\n",
    "\n",
    "an object marked as 'difficult', indicates that the object is considered difficult to recognize,  \n",
    "for example an object which is clearly visible but unidentifiable without substantial use of context.  \n",
    "Objects marked as difficult are currently ignored in the evaluation of the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\n",
    "\n",
    "from datasets.COCO_dataset import COCODataset\n",
    "\n",
    "os.chdir(\"utils\")\n",
    "from compute_average_precision_per_class import compute_average_precision_per_class\n",
    "from group_annotation_by_class import group_annotation_by_class\n",
    "os.chdir(\"..\")\n",
    "\n",
    "\n",
    "print(\"SSD Evaluation on COCO Dataset.\")\n",
    "\n",
    "net = \"vgg16-ssd\"\n",
    "folder = \"/home/user/ssd1/taeholee/SSD/coco-ssd/\"\n",
    "trained_model = folder + \"models/vgg16-ssd-Epoch-3-Loss-20.2706.pth\"\n",
    "label_file = folder + \"models/coco-model-labels.txt\"\n",
    "dataset_type = \"COCO\"\n",
    "dataset = \"/home/user/hdd/coco/\"\n",
    "txt_folder = folder + \"data/\"\n",
    "\n",
    "use_cuda = True\n",
    "use_2007_metric = True # figure out: It computes average precision based on the definition of Pascal Competition.\n",
    "nms_method = \"hard\"\n",
    "iou_threshold = 0.5\n",
    "eval_dir = \"eval_results\"\n",
    "\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    print(\"Use Cuda.\")\n",
    "    DEVICE = torch.device(\"cuda:3\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "if use_2007_metric:\n",
    "    print(\"Computes average precision based on the definition of Pascal Competition.\")\n",
    "\n",
    "#main    \n",
    "eval_path = pathlib.Path(eval_dir)\n",
    "eval_path.mkdir(exist_ok=True)\n",
    "    \n",
    "class_names = [name.strip() for name in open(label_file).readlines()]\n",
    "\n",
    "if dataset_type == \"COCO\":\n",
    "    dataset = COCODataset(root=dataset, txt=txt_folder, is_test=True)\n",
    "else:\n",
    "    print(\"No dataset_type\")\n",
    "\n",
    "print(\"Test dataset size: {}\". format(len(dataset)))\n",
    "true_case_stat, all_gb_boxes, all_difficult_cases = group_annotation_by_class(dataset)\n",
    "    \n",
    "if net == 'vgg16-ssd':\n",
    "    net = create_vgg_ssd(len(class_names), device=DEVICE, is_test=True)\n",
    "    predictor = create_vgg_ssd_predictor(net, nms_method=nms_method, device=DEVICE)\n",
    "else:\n",
    "    print(\"No net type\")\n",
    "\n",
    "print(\"\\nLoad Model:\", trained_model)\n",
    "torch.load(trained_model)\n",
    "#net.load(trained_model)\n",
    "net.to(DEVICE)\n",
    "\n",
    "\n",
    "results = []\n",
    "for i in range(len(dataset)):\n",
    "    if i == 0:\n",
    "        print(\"process image\", i, \"start\")\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        print(\"process image\", i)\n",
    "    if i == (len(dataset)-1):\n",
    "        print(\"process image\", i, \"end\")\n",
    "    \n",
    "    image = dataset.get_image(i)\n",
    "    boxes, labels, probs = predictor.predict(image)\n",
    "    indexes = torch.ones(labels.size(0), 1, dtype=torch.float32) * i\n",
    "    results.append(torch.cat([indexes.reshape(-1, 1),\n",
    "                              labels.reshape(-1, 1).float(),\n",
    "                              probs.reshape(-1, 1),\n",
    "                              boxes + 0.0  # matlab's indexes start from 1\n",
    "                             ], dim=1))\n",
    "        \n",
    "results = torch.cat(results)\n",
    "    \n",
    "aps = []\n",
    "print(\"\\nAverage Precision Per-class:\\n\")\n",
    "for class_index, class_name in enumerate(class_names):\n",
    "    if class_index == 0:\n",
    "        continue  # ignore background\n",
    "        \n",
    "    prediction_path = str(eval_path) + \"/det_test_%s.txt\" %class_name\n",
    "        \n",
    "    with open(prediction_path, \"w\") as f:\n",
    "        sub = results[results[:, 1] == class_index, :]\n",
    "        for i in range(sub.size(0)):\n",
    "            prob_box = sub[i, 2:].numpy()\n",
    "            image_id = dataset.ids[int(sub[i, 0])]\n",
    "            print(image_id + \" \" + \" \".join([str(v) for v in prob_box]), file=f)\n",
    "        \n",
    "    try:\n",
    "        ap = compute_average_precision_per_class(true_case_stat[class_index],\n",
    "                                                 all_gb_boxes[class_index],\n",
    "                                                 all_difficult_cases[class_index],\n",
    "                                                 prediction_path,\n",
    "                                                 iou_threshold,\n",
    "                                                 use_2007_metric)\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "    aps.append(ap)\n",
    "    if ap > 1e-7: #exclude small ap\n",
    "        print(\"%s:\" %class_name, \"%f\" %ap)\n",
    "    \n",
    "print(\"\\nAverage Precision Across All Classes: %f\" %(sum(aps)/len(aps)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('utils')\n",
    "from image_print import image_print\n",
    "os.chdir('..')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "net_type = \"vgg16-ssd\"\n",
    "folder = \"/home/user/ssd1/taeholee/SSD/coco-ssd/\"\n",
    "model_path = folder + \"models/vgg16-ssd-Epoch-3-Loss-20.2706.pth\"\n",
    "label_path = folder + \"models/coco-model-labels.txt\"\n",
    "data_dir = \"/home/user/hdd/coco/\"\n",
    "test_txt = folder + \"data/test.txt\"\n",
    "how_many_images = 2\n",
    "\n",
    "print(\"Use model:\", model_path, \"\\n\")\n",
    "image_print(net_type=net_type,\n",
    "            model_path=model_path,\n",
    "            label_path=label_path,\n",
    "            data_dir=data_dir,\n",
    "            test_txt=test_txt,\n",
    "            how_many_images=how_many_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
